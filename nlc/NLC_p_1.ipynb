{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuhvZ3sXxic1sGGDglrTCI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumyuck/ML-learning/blob/main/nlc/NLC_p_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLC Practical - 1\n",
        "Roll No: 23BCE298\n",
        "\n",
        "Name: Samyak Jain\n",
        "\n",
        "Aim: Introduction to text processing libraries"
      ],
      "metadata": {
        "id": "R1McMS6_hOLj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqFJc3RUbATv",
        "outputId": "84ea0a90-92ef-4a03-b7db-27f11c847f2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found email addresses: ['support@example.com']\n",
            "Found phone number: 123-456-7890\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Example text\n",
        "text = \"Contact us at support@example.com or call us at 123-456-7890.\"\n",
        "\n",
        "# Pattern matching: Find all email addresses in the text\n",
        "email_pattern = r'\\S+@\\S+'\n",
        "emails = re.findall(email_pattern, text)\n",
        "print(\"Found email addresses:\", emails)\n",
        "\n",
        "# Text extraction: Extract the phone number\n",
        "phone_pattern = r'\\d{3}-\\d{3}-\\d{4}'\n",
        "phone_number = re.search(phone_pattern, text)\n",
        "if phone_number:\n",
        "  print(\"Found phone number:\", phone_number.group(0))\n",
        "else:\n",
        "  print(\"Phone number not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data (if you haven't already)\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "text = \"NLTK is a powerful library for natural language processing. It helps with tokenization, stemming, and lemmatization.\"\n",
        "\n",
        "# Tokenization\n",
        "words = word_tokenize(text)\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "print(\"Word tokens:\", words)\n",
        "print(\"Sentence tokens:\", sentences)\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(\"Stemmed words:\", stemmed_words)\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPuRX15CdgrF",
        "outputId": "8797b254-e05f-4ba7-ff4a-1818570721ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokens: ['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', '.', 'It', 'helps', 'with', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', '.']\n",
            "Sentence tokens: ['NLTK is a powerful library for natural language processing.', 'It helps with tokenization, stemming, and lemmatization.']\n",
            "Stemmed words: ['nltk', 'is', 'a', 'power', 'librari', 'for', 'natur', 'languag', 'process', '.', 'it', 'help', 'with', 'token', ',', 'stem', ',', 'and', 'lemmat', '.']\n",
            "Lemmatized words: ['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', '.', 'It', 'help', 'with', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Named Entity Recognition (NER)\n",
        "print(\"Named Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} ({ent.label_})\")\n",
        "\n",
        "# Dependency Parsing\n",
        "print(\"\\nDependency Parsing:\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text} ({token.pos_}) - {token.dep_} -> {token.head.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHD8xqvpePW7",
        "outputId": "475413f7-f37e-4b3f-9669-de48e8b79ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities:\n",
            "Apple (ORG)\n",
            "U.K. (GPE)\n",
            "$1 billion (MONEY)\n",
            "\n",
            "Dependency Parsing:\n",
            "Apple (PROPN) - nsubj -> looking\n",
            "is (AUX) - aux -> looking\n",
            "looking (VERB) - ROOT -> looking\n",
            "at (ADP) - prep -> looking\n",
            "buying (VERB) - pcomp -> at\n",
            "U.K. (PROPN) - nsubj -> startup\n",
            "startup (VERB) - ccomp -> buying\n",
            "for (ADP) - prep -> startup\n",
            "$ (SYM) - quantmod -> billion\n",
            "1 (NUM) - compound -> billion\n",
            "billion (NUM) - pobj -> for\n",
            ". (PUNCT) - punct -> looking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "from googletrans import Translator\n",
        "\n",
        "text = \"TextBlob is a great library for sentiment analysis and translation.\"\n",
        "\n",
        "# Sentiment Analysis\n",
        "blob = TextBlob(text)\n",
        "sentiment = blob.sentiment\n",
        "print(f\"Sentiment of the text: {sentiment}\")\n",
        "\n",
        "# Translation\n",
        "translator = Translator()\n",
        "translated_text = translator.translate(text_to_translate, dest='es')\n",
        "print(f\"Translated text (Spanish): {translated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBPn1k24eobX",
        "outputId": "aab6bfa4-b98c-442c-de00-ed5200348f95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment of the text: Sentiment(polarity=0.8, subjectivity=0.75)\n",
            "Translated text (Spanish): Translated(src=en, dest=es, text=¿Hola, cómo estás?, pronunciation=None, extra_data=\"{'confiden...\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Some Common Pre-Processing Steps"
      ],
      "metadata": {
        "id": "R0MQ6qxSiu6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import required stuff\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import string"
      ],
      "metadata": {
        "id": "PDUB4cMyi3nP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read training data\n",
        "trdf=pd.read_csv('train.csv', header='infer')\n",
        "\n",
        "print(trdf.head(3))\n",
        "trdf.info()"
      ],
      "metadata": {
        "id": "_SUUVOQ2i9l7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45a8c9c8-3ec5-401d-bf0f-8e709d7e85d3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id keyword location                                               text  \\\n",
            "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
            "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
            "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
            "\n",
            "   target  \n",
            "0       1  \n",
            "1       1  \n",
            "2       1  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7613 entries, 0 to 7612\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        7613 non-null   int64 \n",
            " 1   keyword   7552 non-null   object\n",
            " 2   location  5080 non-null   object\n",
            " 3   text      7613 non-null   object\n",
            " 4   target    7613 non-null   int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 297.5+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert everything in text in lower case\n",
        "trdf['lowered_text']=trdf['text'].str.lower()\n",
        "\n",
        "print(trdf['lowered_text'].head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdXlEEsDksQV",
        "outputId": "37e3264c-2b22-4ada-9f80-901f1642aef0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    our deeds are the reason of this #earthquake m...\n",
            "1               forest fire near la ronge sask. canada\n",
            "2    all residents asked to 'shelter in place' are ...\n",
            "Name: lowered_text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Removal of punctuation\n",
        "\n",
        "punctuation=string.punctuation\n",
        "\n",
        "print(type(punctuation), punctuation)\n",
        "\n",
        "mapping=str.maketrans(\"\",\"\",punctuation)\n",
        "\n",
        "print(type(mapping), mapping)\n",
        "\n",
        "print(trdf['lowered_text'].head(10))\n",
        "trdf['lowered_text']=trdf[\"lowered_text\"].str.translate(mapping)\n",
        "print(trdf['lowered_text'].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrV7t0HglSMK",
        "outputId": "5ba92a36-b285-4f37-80d0-e12e2937732d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'> !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "<class 'dict'> {33: None, 34: None, 35: None, 36: None, 37: None, 38: None, 39: None, 40: None, 41: None, 42: None, 43: None, 44: None, 45: None, 46: None, 47: None, 58: None, 59: None, 60: None, 61: None, 62: None, 63: None, 64: None, 91: None, 92: None, 93: None, 94: None, 95: None, 96: None, 123: None, 124: None, 125: None, 126: None}\n",
            "0    our deeds are the reason of this #earthquake m...\n",
            "1               forest fire near la ronge sask. canada\n",
            "2    all residents asked to 'shelter in place' are ...\n",
            "3    13,000 people receive #wildfires evacuation or...\n",
            "4    just got sent this photo from ruby #alaska as ...\n",
            "5    #rockyfire update => california hwy. 20 closed...\n",
            "6    #flood #disaster heavy rain causes flash flood...\n",
            "7    i'm on top of the hill and i can see a fire in...\n",
            "8    there's an emergency evacuation happening now ...\n",
            "9    i'm afraid that the tornado is coming to our a...\n",
            "Name: lowered_text, dtype: object\n",
            "0    our deeds are the reason of this earthquake ma...\n",
            "1                forest fire near la ronge sask canada\n",
            "2    all residents asked to shelter in place are be...\n",
            "3    13000 people receive wildfires evacuation orde...\n",
            "4    just got sent this photo from ruby alaska as s...\n",
            "5    rockyfire update  california hwy 20 closed in ...\n",
            "6    flood disaster heavy rain causes flash floodin...\n",
            "7    im on top of the hill and i can see a fire in ...\n",
            "8    theres an emergency evacuation happening now i...\n",
            "9     im afraid that the tornado is coming to our area\n",
            "Name: lowered_text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let us have a look at standard list of stopwords\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "print(type(stopwords.words()), len(stopwords.words()))\n",
        "\n",
        "\n",
        "print(type(stopwords.words('english')), len(stopwords.words('english'))) #list, 179 stopwords\n",
        "\n",
        "print(stopwords.words('english'))\n",
        "stopwords_eng=stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leCMx3MLlnGv",
        "outputId": "4a35f162-dc02-41e5-dbf4-054387c6f5bf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 11009\n",
            "<class 'list'> 198\n",
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function to remove stop words\n",
        "def remove_stopwords(in_str):\n",
        "    new_str=''\n",
        "    words=in_str.split()\n",
        "    for tx in words:\n",
        "        if tx not in stopwords_eng:\n",
        "            new_str=new_str + tx + \" \"\n",
        "    return new_str\n",
        "\n",
        "trdf['lowered_text_stop_removed']=trdf[\"lowered_text\"].apply(lambda x: remove_stopwords(x))\n",
        "\n",
        "print(trdf[\"lowered_text_stop_removed\"].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxqDwrppmN_c",
        "outputId": "ca803b8e-8880-419b-d91e-53bc6cdfec9f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        deeds reason earthquake may allah forgive us \n",
            "1               forest fire near la ronge sask canada \n",
            "2    residents asked shelter place notified officer...\n",
            "3    13000 people receive wildfires evacuation orde...\n",
            "4    got sent photo ruby alaska smoke wildfires pou...\n",
            "5    rockyfire update california hwy 20 closed dire...\n",
            "6    flood disaster heavy rain causes flash floodin...\n",
            "7                          im top hill see fire woods \n",
            "8    theres emergency evacuation happening building...\n",
            "9                       im afraid tornado coming area \n",
            "Name: lowered_text_stop_removed, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now Stemming using PorterStemmer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "stemmer=PorterStemmer()\n",
        "\n",
        "print(trdf[\"lowered_text_stop_removed\"].head(5))\n",
        "\n",
        "def do_stemming(in_str):\n",
        "    new_str=\"\"\n",
        "    for word in in_str.split():\n",
        "        new_str=new_str + stemmer.stem(word) + \" \"\n",
        "    return new_str\n",
        "\n",
        "trdf[\"Stemmed\"]=trdf[\"lowered_text_stop_removed\"].apply(lambda x: do_stemming(x))\n",
        "\n",
        "print(trdf[\"Stemmed\"].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11Vk6RBAmW5G",
        "outputId": "c7778269-05f4-4c07-eb69-5fbe4465d535"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        deeds reason earthquake may allah forgive us \n",
            "1               forest fire near la ronge sask canada \n",
            "2    residents asked shelter place notified officer...\n",
            "3    13000 people receive wildfires evacuation orde...\n",
            "4    got sent photo ruby alaska smoke wildfires pou...\n",
            "Name: lowered_text_stop_removed, dtype: object\n",
            "0           deed reason earthquak may allah forgiv us \n",
            "1                forest fire near la rong sask canada \n",
            "2    resid ask shelter place notifi offic evacu she...\n",
            "3    13000 peopl receiv wildfir evacu order califor...\n",
            "4    got sent photo rubi alaska smoke wildfir pour ...\n",
            "Name: Stemmed, dtype: object\n"
          ]
        }
      ]
    }
  ]
}